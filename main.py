import os
import time
import feedparser
import urllib.parse
from datetime import datetime, timedelta, timezone
from google import genai
from elevenlabs.client import ElevenLabs
from collections import defaultdict
from urllib.parse import urlparse
from dateutil import parser as date_parser # ë‚ ì§œ íŒŒì‹±ìš©

# 1. í™˜ê²½ ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
VOICE_ID = "cjVigY5qzO86Huf0OWal"

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = genai.Client(api_key=GEMINI_API_KEY)

def parse_date(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜ (ì •ë ¬ìš©)"""
    try:
        return date_parser.parse(date_str)
    except:
        return datetime.now()

from collections import defaultdict
from urllib.parse import urlparse

# ë‚ ì§œ íŒŒì‹± í—¬í¼ í•¨ìˆ˜ (ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì¶”ê°€)
def parse_date(date_str):
    try:
        from dateutil import parser
        return parser.parse(date_str)
    except:
        return datetime.now()

def fetch_news():
    print("ğŸ“¡ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ì •ë°€ í•„í„°ë§ ì¤‘... (ìµœê·¼ 24ì‹œê°„ ì´ë‚´ + 10ê°œ ì œí•œ)")
    
    # í•œêµ­ ì‹œê°„(KST) ê¸°ì¤€ í˜„ì¬ ìš”ì¼ í™•ì¸ (0: ì›”, 1: í™”, ..., 5: í† , 6: ì¼)
    KST = timezone(timedelta(hours=9))
    now_kst = datetime.now(KST)
    weekday = now_kst.weekday()

    # 1. ì¼ìš”ì¼ ë°œí–‰ ì¤‘ë‹¨ ë¡œì§
    if weekday == 6:
        print("ğŸ“… ì˜¤ëŠ˜ì€ ì¼ìš”ì¼ì…ë‹ˆë‹¤. ë¦¬í¬íŠ¸ë¥¼ ë°œí–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    # 2. ìš”ì¼ì— ë”°ë¥¸ ê²€ìƒ‰ ê¸°ê°„(when) ì„¤ì •
    # ì›”ìš”ì¼(0)ì´ë©´ 7ì¼(7d), ê·¸ ì™¸ í‰ì¼ì€ 1ì¼(1d)
    search_period = "7d" if weekday == 0 else "1d"
    print(f"ğŸ“¡ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ê²€ìƒ‰ ê¸°ê°„: {search_period})")
    
    # 2. íƒ€ê²Ÿ ë§¤ì²´ ì„¤ì •
    GLOBAL_TARGETS = {
    "digitimes.com": "Digitimes",
    "electronicsweekly.com": "Electronics Weekly",
    "eetimes.com": "EE Times",
    "trendforce.com": "TrendForce",
    "semiconductor-digest.com": "Semi Digest",
    "semiengineering.com": "Semiconductor Engineering",
    "3dincites.com": "3D InCites",
    "yolegroup.com": "Yole Group",
    "ddaily.co.kr": "Digital Daily"
    }
    KOREA_TARGETS = {
        "thelec.kr": "TheElec",
        "zdnet.co.kr": "ZDNet Korea",
        "dt.co.kr": "Digital Times",
        "hankyung.com": "Hankyung Insight",
        "etnews.com": "ETNews",
        "kipost.net": "KIPOST"
    }
    ALL_TARGETS = {**GLOBAL_TARGETS, **KOREA_TARGETS}

KEYWORDS = [
    # ê¸°ì¡´ í•µì‹¬ í‚¤ì›Œë“œ
    'semiconductor', 'advanced packaging', 'hbm', 'tsmc', 'samsung', 'sk hynix', 'micron', 'hbf',
    'wafer', 'chiplet', 'interposer','intel'
    
    # ê³µì • ë° êµ¬ì¡° í™•ì¥
    'Hybrid Bonding', 'CoWoS', 'FOWLP', 'PLP', '3D IC', 'TSV',
    
    # ì†Œì¬ ë° ì¬ë£Œê°œë°œ (TFT í•µì‹¬)
    'Glass Substrate', 'TC-NCF', 'MUF', 'EMC', 'Substrate material',
    
    # ì°¨ì„¸ëŒ€ ì•„í‚¤í…ì²˜
    'CXL', 'BSPDN', 'UCIe', 'Silicon Photonics', 'Heterogeneous Integration'
    ]
    
# 3. RSS ìˆ˜ì§‘ í•¨ìˆ˜ (search_period ë°˜ì˜)
def fetch_rss(targets, region, lang):
        site_query = " OR ".join([f"site:{d}" for d in targets.keys()])
        kw_query = " OR ".join(KEYWORDS)
        final_query = f"({site_query}) AND ({kw_query})"
        encoded_query = urllib.parse.quote(final_query)
        # ì„¤ì •ëœ ê¸°ê°„(search_period)ì„ URLì— ë°˜ì˜
        url = f"https://news.google.com/rss/search?q={encoded_query}+when:{search_period}&hl={lang}&gl={region}&ceid={region}:{lang}"
        return feedparser.parse(url).entriesentries

    raw_articles = []
    print("   - ê¸€ë¡œë²Œ/êµ­ë‚´ ì†ŒìŠ¤ ìŠ¤ìº” ì¤‘...")
    raw_articles.extend(fetch_rss(GLOBAL_TARGETS, "US", "en-US"))
    raw_articles.extend(fetch_rss(KOREA_TARGETS, "KR", "ko"))

    # 4. [í•µì‹¬] ë‚ ì§œ ê¸°ë°˜ ê°•ì œ í•„í„°ë§ & ì •ì œ
    valid_articles = []
    seen_links = set()
    # --- ì¶”ê°€: ì œì™¸ í‚¤ì›Œë“œ ì„¤ì • (ì£¼ì‹, ì¦ê¶Œ, íˆ¬ì ìœ ë„ ë“±) ---
    EXCLUDE_KEYWORDS = [
        'ì£¼ê°€', 'ì¦ì‹œ', 'ì¢…ëª©', 'ìƒí•œê°€', 'í•˜í•œê°€', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ìˆ˜ìµë¥ ', 
        'ê°œë¯¸', 'ì™¸ì¸', 'ê¸°ê´€', 'í…Œë§ˆì£¼', 'ê¸‰ë“±', 'ê¸‰ë½', 'íˆ¬ìì •ë³´', 'ì¦ê¶Œì‚¬',
        'stock', 'shares', 'trading', 'investment', 'price target', 'buy rating'
    ]
    # ----------------------------------------------------
    print(f"   - 1ì°¨ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(raw_articles)}ê°œ")

    for e in raw_articles:
        if e.link in seen_links: continue

        # URL ì •ì œ: êµ¬ê¸€ ë‰´ìŠ¤ ë¦¬ë””ë ‰ì…˜ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì†Œí™”í•˜ê³  ì•ˆì „í•˜ê²Œ ì¸ì½”ë”©
        original_link = e.link
        # ë§Œì•½ URLì— í•œê¸€ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìê°€ ì„ì—¬ ë¦¬ë””ë ‰ì…˜ ì˜¤ë¥˜ê°€ ë‚œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì²˜ë¦¬
        clean_url = urllib.parse.unquote(original_link).split("&url=")[-1].split("&")[0] if "&url=" in original_link else original_link
    
        # (A) ë‚ ì§œ íŒŒì‹± ë° ê²€ì¦
        try:
            # feedparserê°€ íŒŒì‹±í•´ì¤€ ë‚ ì§œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¬¸ìì—´ íŒŒì‹± ì‹œë„
            if hasattr(e, 'published_parsed') and e.published_parsed:
                # struct_timeì„ datetime ê°ì²´ë¡œ ë³€í™˜
                pub_date = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
            else:
                pub_date = date_parser.parse(e.published)
                # timezone ì •ë³´ê°€ ì—†ìœ¼ë©´ UTCë¡œ ê°€ì •
                if pub_date.tzinfo is None:
                    pub_date = pub_date.replace(tzinfo=timezone.utc)
            
            # (B) 24ì‹œê°„ ì´ë‚´ì¸ì§€ í™•ì¸ (ì˜¤ë˜ëœ ê¸°ì‚¬ ì¦‰ì‹œ íê¸°)
            if pub_date < cutoff_date:
                continue

        except Exception as err:
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ìŠ¤í‚µ (ì˜¤ë˜ëœ ê¸°ì‚¬ì¼ í™•ë¥  ë†’ìŒ)
            continue

        # --- ì¶”ê°€: ìˆœìˆ˜ ë°˜ë„ì²´ ë‰´ìŠ¤ í•„í„°ë§ (ì£¼ì‹ ê´€ë ¨ ë‚´ìš© ì œì™¸) ---
        title = e.title.lower()
        summary = e.summary.lower() if hasattr(e, 'summary') else ""
        
        # ì œì™¸ í‚¤ì›Œë“œê°€ ì œëª©ì´ë‚˜ ìš”ì•½ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        is_stock_news = any(kw in title or kw in summary for kw in EXCLUDE_KEYWORDS)
        
        if is_stock_news:
            # ì£¼ì‹ ê´€ë ¨ ê¸°ì‚¬ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
            continue
        # -------------------------------------------------------
        
        seen_links.add(e.link)
        e['parsed_date'] = pub_date # ì •ë ¬ì„ ìœ„í•´ ì €ì¥
        
        # (C) ì¶œì²˜ëª… ë§¤í•‘
        domain = urlparse(e.link).netloc.replace("www.", "")
        source_name = "News"
        for t_domain, t_name in ALL_TARGETS.items():
            if t_domain in domain:
                source_name = t_name
                break
        if source_name == "News" and hasattr(e, 'source'):
            source_name = e.source.title
        
        e['display_source'] = source_name
        valid_articles.append(e)

        # (C) ì¶œì²˜ëª… ë§¤í•‘ ë¶€ë¶„ì—ì„œ URL ì €ì¥ ì‹œ clean_url ì‚¬ìš©
        e['link'] = original_link # ë˜ëŠ” ì •ì œëœ clean_url

    print(f"   - 24ì‹œê°„ ì´ë‚´ ìœ íš¨ ê¸°ì‚¬: {len(valid_articles)}ê°œ")

    # 5. ë§¤ì²´ë³„ ì¿¼í„°ì œ (ë‹¤ì–‘ì„± í™•ë³´)
    buckets = defaultdict(list)
    for e in valid_articles:
        buckets[e['display_source']].append(e)
    
    # ê° ë²„í‚· ìµœì‹ ìˆœ ì •ë ¬
    for s in buckets:
        buckets[s].sort(key=lambda x: x['parsed_date'], reverse=True)

    final_selection = []
    selected_titles = set()
    
    # ìš°ì„ ìˆœìœ„: ì§€ì • ë§¤ì²´ ë¦¬ìŠ¤íŠ¸ ìˆœì„œëŒ€ë¡œ 1ê°œì”© ë½‘ê¸°
    priority_order = list(ALL_TARGETS.values())
    
    # 1ë¼ìš´ë“œ: ë§¤ì²´ë³„ 1ê°œì”© (ìµœëŒ€ 2ê°œê¹Œì§€ í—ˆìš©)
    for _ in range(2): # ìµœëŒ€ 2ë°”í€´ë¥¼ ë•ë‹ˆë‹¤.
        for source_name in priority_order:
            if buckets[source_name]:
                article = buckets[source_name].pop(0)
                if article.title not in selected_titles:
                    final_selection.append(article)
                    selected_titles.add(article.title)
            if len(final_selection) >= 10: break
        if len(final_selection) >= 10: break

    # ë§Œì•½ 10ê°œê°€ ì•ˆ ì±„ì›Œì¡Œë‹¤ë©´ ë‚˜ë¨¸ì§€ì—ì„œ ìµœì‹ ìˆœìœ¼ë¡œ ë³´ì¶©
    if len(final_selection) < 10:
        remaining = []
        for s_list in buckets.values(): remaining.extend(s_list)
        remaining.sort(key=lambda x: x['parsed_date'], reverse=True)
        for article in remaining:
            if len(final_selection) >= 10: break
            if article.title not in selected_titles:
                final_selection.append(article)
                selected_titles.add(article.title)

    # [í•µì‹¬] URL ë¦¬ë””ë ‰ì…˜ í•´ê²°ì„ ìœ„í•´ google news ë§í¬ ëŒ€ì‹  'clean_url' ì „ë‹¬ ë¡œì§ í™•ì¸
    # RSSì—ì„œ ì œê³µí•˜ëŠ” linkê°€ ê°€ë” ì¸ì½”ë”© ì´ìŠˆë¥¼ ì¼ìœ¼í‚¤ë¯€ë¡œ 
    # í”„ë¡¬í”„íŠ¸ì—ì„œ HTML <a> íƒœê·¸ í˜•ì‹ì„ ì§ì ‘ ì“°ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
    return final_selection # ê°ì²´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•˜ì—¬ generate_contentì— ì „ë‹¬

    # 2ë¼ìš´ë“œ: ë‚¨ì€ ê¸°ì‚¬ ì¤‘ ìµœì‹ ìˆœìœ¼ë¡œ ì±„ìš°ê¸°
    remaining = []
    for source_list in buckets.values():
        remaining.extend(source_list)
    remaining.sort(key=lambda x: x['parsed_date'], reverse=True)

    # â˜… 10ê°œ ì œí•œ ì„¤ì •
    TARGET_COUNT = 10
    
    for article in remaining:
        if len(final_selection) >= TARGET_COUNT: break
        if article.title not in selected_titles:
            final_selection.append(article)
            selected_titles.add(article.title)

    # 6. ìµœì¢… í…ìŠ¤íŠ¸ ìƒì„±
    formatted_text = []
    # ê²°ê³¼ ë³´ì—¬ì¤„ ë•Œë„ ìµœì‹ ìˆœ ì •ë ¬
    final_selection.sort(key=lambda x: x['parsed_date'], reverse=True)

    for i, e in enumerate(final_selection):
        clean_summ = e.summary.replace("<b>", "").replace("</b>", "").replace("&nbsp;", " ") if hasattr(e, 'summary') else ""
        
        # AI í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°ˆ í¬ë§·
        item = (
            f"[{i+1}] Source: {e['display_source']}\n"
            f"Date: {e['parsed_date'].strftime('%Y-%m-%d %H:%M')}\n"
            f"Title: {e.title}\n"
            f"URL: {e.link}\n"
            f"Summary: {clean_summ[:300]}\n"
        )
        formatted_text.append(item)

    if not formatted_text:
        return "ìµœê·¼ 24ì‹œê°„ ì´ë‚´ì˜ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

    print(f"âœ… ìµœì¢… ì„ ë³„ ì™„ë£Œ: {len(formatted_text)}ê°œ (10ê°œ ì œí•œ, 24ì‹œê°„ ì´ë‚´ ì—„ìˆ˜)")
    return "\n".join(formatted_text)

def generate_content(news_text):
    """Geminië¥¼ ì´ìš©í•´ ë‰´ìŠ¤ë ˆí„°ì™€ ë¼ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    print("ğŸ¤– AI ë¶„ì„ ë° ì§‘í•„ ì¤‘... (ê°€ë…ì„± ìµœì í™” ëª¨ë“œ)")
    # í•œêµ­ ì‹œê°„(KST, UTC+9) ì„¤ì •
    KST = timezone(timedelta(hours=9))
    today_date = datetime.now(KST).strftime("%Yë…„ %mì›” %dì¼")
    publisher = "ë°˜ë„ì²´ì¬ë£Œê°œë°œTFT ê¹€ë™íœ˜"
  
    # í”„ë¡¬í”„íŠ¸ ì„¤ê³„
    prompt = f"""
    ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì‚°ì—… ìˆ˜ì„ ì „ëµê°€ì´ì ì¸ê¸° í…Œí¬ ë‰´ìŠ¤ë ˆí„° ë°œí–‰ì¸ì…ë‹ˆë‹¤.
    ì˜¤ëŠ˜ ë‚ ì§œëŠ” {today_date}, ë°œí–‰ì¸ì€ '{publisher}'ì…ë‹ˆë‹¤.
    
    ---
    ì œê³µëœ [ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°]ì— í¬í•¨ë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ ì°½ì‘í•˜ê±°ë‚˜ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ë‰´ìŠ¤ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ 'ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ'ì´ë¼ê³  ëª…ì‹œí•˜ì‹­ì‹œì˜¤.
    í•´ë‹¹ ë‚ ì§œ ê¸°ì¤€ 24ì‹œê°„ ì´ë‚´ì˜ ê¸°ì‚¬ë¥¼ ì¸ìš©í•˜ë¯€ë¡œ, ë¬´ì¡°ê±´ ê³µê°œëœ ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ ê²€ì¦í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    
    - ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    - **ê°€ë…ì„± ì›ì¹™**: ì¤„ê¸€ë¡œ ê¸¸ê²Œ ì“°ì§€ ë§ê³ , ë¶ˆë › í¬ì¸íŠ¸ì™€ ë³¼ë“œì²´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
    **ì¤‘ìš”: ì¶œì²˜ í‘œê¸° ë°©ì‹ (Hyperlink)**
    - ê° ë‰´ìŠ¤ í•­ëª©ì˜ ëì—ëŠ” ë°˜ë“œì‹œ ì›ë³¸ ê¸°ì‚¬ë¡œ ì´ë™í•˜ëŠ” ë§í¬ë¥¼ ê±¸ì–´ì•¼ í•©ë‹ˆë‹¤.
    - í˜•ì‹: `[ì¶œì²˜: [ì–¸ë¡ ì‚¬ëª…](ê¸°ì‚¬URL)]`
    - ì˜ˆì‹œ: `...ì „ë§ì…ë‹ˆë‹¤. [ì¶œì²˜: [Digitimes](https://www.digitimes.com/...)]`
    - ì œê³µëœ ë°ì´í„°ì˜ 'URL' í•„ë“œ ê°’ì„ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”. ê°€ì§œ ë§í¬ë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
    - ì–¸ë¡ ì‚¬ëª…ì€ ì˜ì–´ë©´ ì˜ì–´, í•œê¸€ì´ë©´ í•œê¸€ ê·¸ëŒ€ë¡œ í‘œê¸°í•˜ì„¸ìš”.
    
    **1. í—¤ë” (Header)**
    - `# ğŸ“¦ ì˜¤ëŠ˜ì˜ ë°˜ë„ì²´ ë‰´ìŠ¤` (ì œëª©)
    - `##### {today_date} | ë°œí–‰ì¸: ë°˜ë„ì²´ì¬ë£Œê°œë°œTFT ê¹€ë™íœ˜` (ë‚ ì§œ ë° ë°œí–‰ì¸, ì‘ê²Œ)
    - êµ¬ë¶„ì„ (`---`) ì‚½ì…   
    
    **2. Executive Summary (ìš”ì•½)**
    - `### ğŸ’¡ Executive Summary`
    - ì „ì²´ ì‹œì¥ íë¦„ì„ 5ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”. í•µì‹¬ í‚¤ì›Œë“œëŠ” **ë³¼ë“œì²´**ë¡œ ê°•ì¡°í•˜ì„¸ìš”.
    
    **3. Key Insights (í•µì‹¬ ë‰´ìŠ¤)**
    - `### ğŸŒ Market & Tech Insights`
    - ìˆ˜ì§‘ëœ ë°ì´í„° ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ë¥¼ ê¼­ 10ê°œ ì„ ì •í•´ì£¼ì„¸ìš”. "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"ê³¼ ê°™ì€ ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    - **ê° í•­ëª© ì‘ì„± í¬ë§· (ì—„ìˆ˜)**:
        **|ê¸°ì—… ë˜ëŠ” ì—…ì²´ëª…|ë‰´ìŠ¤ ì œëª©**
          ë‰´ìŠ¤ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”. *[ì¶œì²˜: ì–¸ë¡ ì‚¬ëª…]* (ê¸°ìš¸ì„ê¼´) ì¨ì£¼ì„¸ìš”.
          í•­ëª© ê°„ í•œ ì¤„ ë„ì›ë‹ˆë‹¤.
    
    **4. Technical Term (ìš©ì–´ í•´ì„¤)**
    - `### ğŸ“š Technical Term`
    - **[ìš©ì–´ëª… (í•œê¸€/ì˜ì–´)]**
    - Technical Term: 'BSPDN', 'Glass Substrate', 'Hybrid Bonding' ë“± ë°˜ë„ì²´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì‹¬ë„ ìˆëŠ” ê¸°ìˆ  ìš©ì–´ 1ê°œë¥¼ ì„ ì •í•´ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”.
    
    ---
    
    - êµ¬ë¶„ì `|ë¼ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸|`ë¥¼ ë¨¼ì € ì ê³  ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.
    - **ì˜¤í”„ë‹**: "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ë„ì²´ì¬ë£Œê°œë°œTFT ê¹€ë™íœ˜ì…ë‹ˆë‹¤. {today_date}, ì˜¤ëŠ˜ ì•„ì¹¨ í™•ì¸í•´ì•¼ í•  ë°˜ë„ì²´ íŒ¨í‚¤ì§• ì£¼ìš” ì†Œì‹ ì „í•´ë“œë¦½ë‹ˆë‹¤."
    - **ë³¸ë¬¸**: ë‰´ìŠ¤ë ˆí„°ì˜ í•µì‹¬ë§Œ ìš”ì•½í•˜ì—¬ 40ì´ˆ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    - **ì–´ì¡°**: "ìµœê·¼ ~ë¼ëŠ” ì†Œì‹ì…ë‹ˆë‹¤.", "~í•  ì „ë§ì…ë‹ˆë‹¤." ë“± ì°¨ë¶„í•˜ê³  ì‹ ë¢°ê° ìˆëŠ” ë‰´ìŠ¤ ë¸Œë¦¬í•‘ í†¤(í•˜ì‹­ì‹œì˜¤ì²´ ìœ„ì£¼)ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    - ì§€ì‹œë¬¸(BGM ë“±)ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    - ë§ˆì§€ë§‰ ë¬¸êµ¬: ë³´ê³ ì„œì˜ ë§¨ ë§ˆì§€ë§‰ì€ ë°˜ë“œì‹œ "ì˜¤ëŠ˜ë„ ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì‹œê¸° ë°”ëë‹ˆë‹¤."ë¡œ ëë§ºìŒ í•˜ì„¸ìš”.    
    ---
    
    [ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°]:
    {news_text}
    """
    
    # ì¡°íšŒëœ ëª¨ë¸ ì¤‘ í…ìŠ¤íŠ¸ ìƒì„±ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ
    available_models = [
        "gemini-2.5-flash",
        "gemini-2.5-pro",
        "gemini-2.0-flash",
        "gemini-flash-latest",
        "gemini-pro-latest"
    ]
    
    for model_name in available_models:
        try:
            print(f"\nê³µì • ì‹œë„ ì¤‘: {model_name}...")
            
            response = client.models.generate_content(
                model=model_name,
                contents=prompt
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if response and hasattr(response, 'text') and response.text:
                print(f"âœ… {model_name} ê°€ë™ ì„±ê³µ!")
                return response.text
            
            # ëŒ€ì²´ ì‘ë‹µ êµ¬ì¡°
            if response and hasattr(response, 'candidates') and response.candidates:
                if hasattr(response.candidates[0], 'content'):
                    text = response.candidates[0].content.parts[0].text
                    print(f"âœ… {model_name} ê°€ë™ ì„±ê³µ!")
                    return text
                
        except Exception as e:
            error_msg = str(e)[:300]
            print(f"âŒ {model_name} ê°€ë™ ì‹¤íŒ¨: {error_msg}")
            time.sleep(1)
            continue
    
    raise Exception("ëª¨ë“  ì—”ì§„ì´ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. API í‚¤ì™€ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")

def generate_audio(script):
    try:
        if not ELEVENLABS_API_KEY or ELEVENLABS_API_KEY == "":
            print("âš ï¸ ElevenLabs API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ GitHub Settings â†’ Secrets â†’ Actions â†’ ELEVENLABS_API_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
            print("ğŸ’¡ https://elevenlabs.io ì—ì„œ API í‚¤ë¥¼ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ™ï¸ ìŒì„± ìƒì„± ì¤‘... (ìŠ¤í¬ë¦½íŠ¸ ê¸¸ì´: {len(script)} ë¬¸ì)")
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ë‚´ê¸° (ElevenLabs ì œí•œ ê³ ë ¤)
        max_chars = 5000
        if len(script) > max_chars:
            print(f"âš ï¸ ìŠ¤í¬ë¦½íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. {max_chars}ìë¡œ ì œí•œí•©ë‹ˆë‹¤.")
            script = script[:max_chars]
        
        el_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)
        
        # ìµœì‹  API ë°©ì‹: text_to_speech.convert ì‚¬ìš©
        audio_generator = el_client.text_to_speech.convert(
            voice_id=VOICE_ID,
            text=script,
            model_id="eleven_multilingual_v2"
        )
        
        # MP3 íŒŒì¼ë¡œ ì €ì¥
        with open("radio.mp3", "wb") as f:
            for chunk in audio_generator:
                if chunk:
                    f.write(chunk)
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        if os.path.exists("radio.mp3"):
            file_size = os.path.getsize("radio.mp3")
            print(f"âœ… ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ! (íŒŒì¼ í¬ê¸°: {file_size:,} ë°”ì´íŠ¸)")
        else:
            print("âš ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ElevenLabs API í‚¤, í• ë‹¹ëŸ‰, ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")

def save_newsletter(content):
    import os
    KST = timezone(timedelta(hours=9))
    date_str = datetime.now(KST).strftime("%Y-%m-%d")
    
    # 1. ë‚ ì§œë³„ ì €ì¥ í´ë” ê²½ë¡œ ì„¤ì • (ì˜ˆ: newsletter/2026-01-29)
    folder_path = f"newsletter/{date_str}"
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)

    # 2. ì˜¤ë””ì˜¤ íŒŒì¼ ì´ë™ (í´ë” ì•ˆìœ¼ë¡œ)
    audio_filename = "radio.mp3"
    audio_path = os.path.join(folder_path, audio_filename)
    if os.path.exists("radio.mp3"):
        os.rename("radio.mp3", audio_path)
    
    # 3. ë‰´ìŠ¤ë ˆí„° ë‚´ìš©ì— ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ê²½ë¡œ ìˆ˜ì •
    # ë°°í¬ìš© index.mdì—ì„œ ì´ íŒŒì¼ì„ ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    audio_player = ""
    if os.path.exists(audio_path):
        audio_player = f"<audio controls style='width: 100%;'><source src='{folder_path}/{audio_filename}' type='audio/mpeg'></audio>\n\n---\n\n"

    # 4. ì•„ì¹´ì´ë¹™ìš© íŒŒì¼ ì €ì¥ (í´ë” ë‚´ë¶€)
    with open(os.path.join(folder_path, "index.md"), "w", encoding="utf-8") as f:
        f.write(content)

    # 5. ìµœì‹  ë°°í¬ìš© íŒŒì¼ ì €ì¥ (ì €ì¥ì†Œ ìµœìƒìœ„ ë£¨íŠ¸)
    # GitHub PagesëŠ” ë³´í†µ ë£¨íŠ¸ì˜ index.mdë¥¼ ì²« í™”ë©´ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    with open("index.md", "w", encoding="utf-8") as f:
        f.write(audio_player + content)

if __name__ == "__main__":
    print("ğŸš€ ë°˜ë„ì²´ ë¦¬í¬íŠ¸ ìƒì‚° ê³µì • ê°œì‹œ\n")
    try:
        raw_data = fetch_news()
        
        # ì¼ìš”ì¼ì´ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì¢…ë£Œ
        if raw_data is None:
            print("ğŸ›‘ ë°œí–‰ ì¡°ê±´ ë¯¸ì¶©ì¡±(ì¼ìš”ì¼ ë“±)ìœ¼ë¡œ ê³µì •ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit(0) 

        # ì›”ìš”ì¼ ì£¼ê°„ ë‰´ìŠ¤ ëŒ€ì‘ì„ ìœ„í•œ ë°ì´í„° í¬ë§·íŒ…
        if isinstance(raw_data, list):
            # ë‰´ìŠ¤ ê°œìˆ˜ê°€ 10ê°œë³´ë‹¤ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì¢… ì„ ë³„ëœ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
            formatted_news = []
            for i, e in enumerate(raw_data[:10]): # ìµœëŒ€ 10ê°œ ì œí•œ
                clean_summ = e.summary.replace("<b>", "").replace("</b>", "") if hasattr(e, 'summary') else ""
                item = (
                    f"[{i+1}] Source: {e['display_source']}\n"
                    f"Date: {e['parsed_date'].strftime('%Y-%m-%d %H:%M')}\n"
                    f"Title: {e.title}\n"
                    f"URL: {e.link}\n"
                    f"Summary: {clean_summ[:300]}\n"
                )
                formatted_news.append(item)
            news_text = "\n".join(formatted_news)
        else:
            news_text = raw_data

        # AI ì»¨í…ì¸  ìƒì„± ë° ì´í›„ ê³µì • ì§„í–‰
        full_text = generate_content(news_text)
        
        print(f"âœ… {len(full_text)} ë°”ì´íŠ¸ì˜ ì»¨í…ì¸  ìƒì„± ì™„ë£Œ")
        
        # ë¼ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ
        if "ë¼ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸" in full_text:
            script = full_text.split("ë¼ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸")[-1].strip()
            print(f"âœ… ë¼ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(script)} ë¬¸ì)")
        else:
            script = full_text[:500]
            print(f"âš ï¸ 'ë¼ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•´ ì²˜ìŒ 500ì ì‚¬ìš©")
        
        print("\nğŸ™ï¸ AI ë¼ë””ì˜¤ ìŒì„± í•©ì„± ì¤‘...")
        generate_audio(script)
        
        print("\nğŸ“ ë‰´ìŠ¤ë ˆí„° ë§ˆí¬ë‹¤ìš´ ìƒì„± ì¤‘...")
        save_newsletter(full_text)
        print("\nâœ…âœ…âœ… ëª¨ë“  ê³µì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! âœ…âœ…âœ…")
    except Exception as error:
        print(f"\nâš ï¸ ì‹œìŠ¤í…œ ê²½ë³´: {error}")
        raise error



